{
  "abstract": {
    "en": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks...",
    "zh": "主要的序列转换模型基1111于..."
  },
  "keywords": [
    "Transformer",
    "Attention Mechanism",
    "Neural Networks"
  ],
  "sections": [
    {
      "id": "section-1",
      "number": "1",
      "title": {
        "en": "Introduction",
        "zh": "引言"
      },
      "content": [
        {
          "id": "block-p1",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Recurrent neural networks, long short-term memory "
              },
              {
                "type": "citation",
                "referenceIds": [
                  "ref-1",
                  "ref-2"
                ],
                "displayText": "[1, 2]"
              },
              {
                "type": "text",
                "content": " and gated recurrent "
              },
              {
                "type": "citation",
                "referenceIds": [
                  "ref-3"
                ],
                "displayText": "[3]"
              },
              {
                "type": "text",
                "content": " neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation "
              },
              {
                "type": "citation",
                "referenceIds": [
                  "ref-4",
                  "ref-5",
                  "ref-6"
                ],
                "displayText": "[4, 5, 6]"
              },
              {
                "type": "text",
                "content": "."
              }
            ],
            "zh": [
              {
                "type": "text",
                "content": "循环神经网络，特别是长短期记忆网络 "
              },
              {
                "type": "citation",
                "referenceIds": [
                  "ref-1",
                  "ref-2"
                ],
                "displayText": "[1, 2]"
              },
              {
                "type": "text",
                "content": " 和门控循环神经网络 "
              },
              {
                "type": "citation",
                "referenceIds": [
                  "ref-3"
                ],
                "displayText": "[3]"
              },
              {
                "type": "text",
                "content": "，已经被牢固地确立为序列建模和转换问题的最先进方法。"
              }
            ]
          }
        },
        {
          "id": "block-p1-2",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Recurrent models typically factor computation along the symbol positions of the input and output sequences. The sequential nature fundamentally limits the parallelization of training examples"
              },
              {
                "type": "footnote",
                "id": "fn-1",
                "content": "This limitation becomes critical at longer sequence lengths, as memory constraints limit batching across examples.",
                "displayText": "¹"
              },
              {
                "type": "text",
                "content": ", which becomes critical at longer sequence lengths. For more details, see "
              },
              {
                "type": "link",
                "url": "https://arxiv.org/abs/1609.08144",
                "title": "Google's Neural Machine Translation",
                "children": [
                  {
                    "type": "text",
                    "content": "the GNMT paper"
                  }
                ]
              },
              {
                "type": "text",
                "content": "."
              }
            ]
          }
        },
        {
          "id": "block-p2",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "The Transformer model architecture is shown in "
              },
              {
                "type": "figure-ref",
                "figureId": "block-fig-1",
                "displayText": "Figure 1"
              },
              {
                "type": "text",
                "content": ". As we will demonstrate in "
              },
              {
                "type": "section-ref",
                "sectionId": "section-3",
                "displayText": "Section 3"
              },
              {
                "type": "text",
                "content": ", the attention mechanism is computed as shown in "
              },
              {
                "type": "equation-ref",
                "equationId": "block-math-1",
                "displayText": "Equation 1"
              },
              {
                "type": "text",
                "content": "."
              }
            ],
            "zh": [
              {
                "type": "text",
                "content": "Transformer模型架构如"
              },
              {
                "type": "figure-ref",
                "figureId": "block-fig-1",
                "displayText": "图1"
              },
              {
                "type": "text",
                "content": "所示。"
              }
            ]
          }
        },
        {
          "id": "block-quote-1",
          "type": "quote",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences."
              }
            ]
          },
          "author": "Bahdanau et al., 2015"
        }
      ]
    },
    {
      "id": "section-2",
      "number": "2",
      "title": {
        "en": "Model Architecture",
        "zh": "模型架构"
      },
      "content": [
        {
          "id": "block-fig-1",
          "type": "figure",
          "number": 1,
          "src": "/api/uploads/images/1fa75ce2-215b-4601-8cf4-17f7ed69f1b8/test.jpeg",
          "alt": "Transformer Architecture",
          "caption": {
            "en": [
              {
                "type": "text",
                "content": "The Transformer model architecture"
              }
            ],
            "zh": [
              {
                "type": "text",
                "content": "Transformer模型架构"
              }
            ]
          },
          "description": {
            "en": [
              {
                "type": "text",
                "content": "The encoder is on the left and the decoder is on the right. The model consists of stacked self-attention and point-wise, fully connected layers."
              }
            ],
            "zh": [
              {
                "type": "text",
                "content": "编码器在左侧，解码器在右侧。该模型由堆叠的自注意力层和逐点全连接层组成。"
              }
            ]
          },
          "width": "80%"
        },
        {
          "id": "block-p3",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Most competitive neural sequence transduction models have an encoder-decoder structure "
              },
              {
                "type": "citation",
                "referenceIds": [
                  "ref-4",
                  "ref-5"
                ],
                "displayText": "[4, 5]"
              },
              {
                "type": "text",
                "content": ". The encoder maps an input sequence of symbol representations "
              },
              {
                "type": "inline-math",
                "latex": "(x_1, ..., x_n)"
              },
              {
                "type": "text",
                "content": " to a sequence of continuous representations "
              },
              {
                "type": "inline-math",
                "latex": "\\mathbf{z} = (z_1, ..., z_n)"
              },
              {
                "type": "text",
                "content": "."
              }
            ]
          }
        },
        {
          "id": "block-divider-1",
          "type": "divider"
        },
        {
          "id": "block-heading-2-1",
          "type": "heading",
          "level": 3,
          "number": "2.1",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Encoder and Decoder Stacks"
              }
            ],
            "zh": [
              {
                "type": "text",
                "content": "编码器和解码器堆栈"
              }
            ]
          }
        },
        {
          "id": "block-p4",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "The encoder is composed of a stack of "
              },
              {
                "type": "inline-math",
                "latex": "N = 6"
              },
              {
                "type": "text",
                "content": " identical layers. Each layer has two sub-layers:"
              }
            ]
          }
        },
        {
          "id": "block-list-1",
          "type": "ordered-list",
          "items": [
            {
              "content": {
                "en": [
                  {
                    "type": "text",
                    "content": "A multi-head self-attention mechanism"
                  }
                ]
              }
            },
            {
              "content": {
                "en": [
                  {
                    "type": "text",
                    "content": "A simple, position-wise fully connected feed-forward network"
                  }
                ]
              }
            }
          ],
          "start": 1
        },
        {
          "id": "block-p5",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Key architectural features include:"
              }
            ]
          }
        },
        {
          "id": "block-list-2",
          "type": "unordered-list",
          "items": [
            {
              "content": {
                "en": [
                  {
                    "type": "text",
                    "content": "Residual connections around each sub-layer"
                  }
                ]
              }
            },
            {
              "content": {
                "en": [
                  {
                    "type": "text",
                    "content": "Layer normalization following each residual connection"
                  }
                ]
              }
            },
            {
              "content": {
                "en": [
                  {
                    "type": "text",
                    "content": "Fixed model dimension "
                  },
                  {
                    "type": "inline-math",
                    "latex": "d_{model} = 512"
                  }
                ]
              }
            }
          ]
        }
      ]
    },
    {
      "id": "section-3",
      "number": "3",
      "title": {
        "en": "Attention",
        "zh": "注意力机制"
      },
      "content": [
        {
          "id": "block-p6",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "An attention function can be described as mapping a query and a set of key-value pairs to an output. The attention mechanism we use is called ",
                "style": {}
              },
              {
                "type": "text",
                "content": "Scaled Dot-Product Attention",
                "style": {
                  "italic": true
                }
              },
              {
                "type": "text",
                "content": ":"
              }
            ]
          }
        },
        {
          "id": "block-math-1",
          "type": "math",
          "latex": "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",
          "label": "(1)",
          "number": 1
        },
        {
          "id": "block-p7",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "where "
              },
              {
                "type": "inline-math",
                "latex": "d_k"
              },
              {
                "type": "text",
                "content": " is the dimension of the keys. We scale by "
              },
              {
                "type": "inline-math",
                "latex": "\\frac{1}{\\sqrt{d_k}}"
              },
              {
                "type": "text",
                "content": " to prevent the dot products from growing too large"
              },
              {
                "type": "footnote",
                "id": "fn-2",
                "content": "For large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.",
                "displayText": "²"
              },
              {
                "type": "text",
                "content": "."
              }
            ]
          }
        },
        {
          "id": "block-heading-3-1",
          "type": "heading",
          "level": 3,
          "number": "3.1",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Multi-Head Attention"
              }
            ]
          }
        },
        {
          "id": "block-p8",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Multi-head attention allows the model to jointly attend to information from different representation subspaces. The formula is:"
              }
            ]
          }
        },
        {
          "id": "block-math-2",
          "type": "math",
          "latex": "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O",
          "label": "(2)",
          "number": 2
        },
        {
          "id": "block-math-3",
          "type": "math",
          "latex": "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)",
          "label": "(3)",
          "number": 3
        },
        {
          "id": "block-table-1",
          "type": "table",
          "number": 1,
          "caption": {
            "en": [
              {
                "type": "text",
                "content": "Model hyperparameters for the Transformer"
              }
            ],
            "zh": [
              {
                "type": "text",
                "content": "Transformer的模型超参数"
              }
            ]
          },
          "description": {
            "en": [
              {
                "type": "text",
                "content": "The table shows the main hyperparameters used in our experiments. See "
              },
              {
                "type": "section-ref",
                "sectionId": "section-5",
                "displayText": "Section 5"
              },
              {
                "type": "text",
                "content": " for training details."
              }
            ]
          },
          "headers": [
            "Parameter",
            "Value",
            "Description"
          ],
          "rows": [
            [
              "N (layers)",
              "6",
              "Number of encoder/decoder layers"
            ],
            [
              "d_model",
              "512",
              "Model dimension"
            ],
            [
              "d_ff",
              "2048",
              "Feed-forward dimension"
            ],
            [
              "h (heads)",
              "8",
              "Number of attention heads"
            ],
            [
              "d_k = d_v",
              "64",
              "Dimension per head"
            ],
            [
              "P_drop",
              "0.1",
              "Dropout probability"
            ]
          ],
          "align": [
            "left",
            "center",
            "left"
          ]
        },
        {
          "id": "block-p9",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "The hyperparameters are summarized in "
              },
              {
                "type": "table-ref",
                "tableId": "block-table-1",
                "displayText": "Table 1"
              },
              {
                "type": "text",
                "content": ". In our work, we use "
              },
              {
                "type": "inline-math",
                "latex": "h = 8"
              },
              {
                "type": "text",
                "content": " parallel attention heads."
              }
            ]
          }
        }
      ]
    },
    {
      "id": "section-4",
      "number": "4",
      "title": {
        "en": "Position-wise Feed-Forward Networks",
        "zh": "逐位置前馈网络"
      },
      "content": [
        {
          "id": "block-p10",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Each layer contains a fully connected feed-forward network applied to each position separately and identically:"
              }
            ]
          }
        },
        {
          "id": "block-math-4",
          "type": "math",
          "latex": "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2",
          "label": "(4)",
          "number": 4
        },
        {
          "id": "block-code-1",
          "type": "code",
          "language": "python",
          "caption": {
            "en": [
              {
                "type": "text",
                "content": "PyTorch implementation of position-wise feed-forward network"
              }
            ]
          },
          "showLineNumbers": true,
          "code": "class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
        }
      ]
    },
    {
      "id": "section-5",
      "number": "5",
      "title": {
        "en": "Training",
        "zh": "训练"
      },
      "content": [
        {
          "id": "block-p11",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "This section describes the training regime for our models, including optimizer settings, regularization techniques, and learning rate schedules."
              }
            ]
          }
        },
        {
          "id": "block-heading-5-1",
          "type": "heading",
          "level": 3,
          "number": "5.1",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "Training Data and Batching"
              }
            ]
          }
        },
        {
          "id": "block-p12",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs "
              },
              {
                "type": "citation",
                "referenceIds": [
                  "ref-7"
                ],
                "displayText": "[7]"
              },
              {
                "type": "text",
                "content": ". Sentences were encoded using byte-pair encoding "
              },
              {
                "type": "citation",
                "referenceIds": [
                  "ref-8"
                ],
                "displayText": "[8]"
              },
              {
                "type": "text",
                "content": "."
              }
            ]
          }
        }
      ]
    },
    {
      "id": "section-6",
      "number": "6",
      "title": {
        "en": "Results",
        "zh": "结果"
      },
      "content": [
        {
          "id": "block-table-2",
          "type": "table",
          "number": 2,
          "caption": {
            "en": [
              {
                "type": "text",
                "content": "BLEU scores on WMT 2014 English-to-German and English-to-French translation tasks"
              }
            ]
          },
          "headers": [
            "Model",
            "EN-DE",
            "EN-FR",
            "Training Cost (FLOPs)"
          ],
          "rows": [
            [
              "ByteNet",
              "23.75",
              "-",
              "-"
            ],
            [
              "Deep-Att + PosUnk",
              "-",
              "39.2",
              "-"
            ],
            [
              "GNMT + RL",
              "24.6",
              "39.92",
              "2.3×10¹⁹"
            ],
            [
              "ConvS2S",
              "25.16",
              "40.46",
              "9.6×10¹⁸"
            ],
            [
              "Transformer (base)",
              "27.3",
              "38.1",
              "3.3×10¹⁸"
            ],
            [
              "Transformer (big)",
              "28.4",
              "41.8",
              "2.3×10¹⁹"
            ]
          ],
          "align": [
            "left",
            "center",
            "center",
            "center"
          ]
        },
        {
          "id": "block-p13",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "As shown in "
              },
              {
                "type": "table-ref",
                "tableId": "block-table-2",
                "displayText": "Table 2"
              },
              {
                "type": "text",
                "content": ", our model achieves ",
                "style": {}
              },
              {
                "type": "text",
                "content": "state-of-the-art results",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "content": " on both translation tasks while requiring significantly less training time."
              }
            ]
          }
        }
      ]
    },
    {
      "id": "section-7",
      "number": "7",
      "title": {
        "en": "Conclusion",
        "zh": "结论"
      },
      "content": [
        {
          "id": "block-p14",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention. We showed that the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. The code is available at "
              },
              {
                "type": "link",
                "url": "https://github.com/tensorflow/tensor2tensor",
                "children": [
                  {
                    "type": "text",
                    "content": "https://github.com/tensorflow/tensor2tensor"
                  }
                ]
              },
              {
                "type": "text",
                "content": "."
              }
            ]
          }
        },
        {
          "id": "block-p15",
          "type": "paragraph",
          "content": {
            "en": [
              {
                "type": "text",
                "content": "We are excited about the future of attention-based models and plan to apply them to other tasks. We also plan to investigate ",
                "style": {}
              },
              {
                "type": "text",
                "content": "local, restricted attention mechanisms",
                "style": {
                  "underline": true
                }
              },
              {
                "type": "text",
                "content": " to efficiently handle very long sequences."
              }
            ]
          }
        }
      ]
    }
  ],
  "references": [
    {
      "id": "ref-1",
      "number": 1,
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "title": "Long Short-Term Memory",
      "publication": "Neural Computation",
      "year": 1997,
      "volume": "9",
      "issue": "8",
      "pages": "1735-1780",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "id": "ref-2",
      "number": 2,
      "authors": [
        "Alex Graves"
      ],
      "title": "Generating Sequences With Recurrent Neural Networks",
      "year": 2013,
      "url": "https://arxiv.org/abs/1308.0850"
    },
    {
      "id": "ref-3",
      "number": 3,
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "KyungHyun Cho",
        "Yoshua Bengio"
      ],
      "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "year": 2014,
      "url": "https://arxiv.org/abs/1412.3555"
    },
    {
      "id": "ref-4",
      "number": 4,
      "authors": [
        "Ilya Sutskever",
        "Oriol Vinyals",
        "Quoc V. Le"
      ],
      "title": "Sequence to Sequence Learning with Neural Networks",
      "publication": "NIPS",
      "year": 2014,
      "pages": "3104-3112"
    },
    {
      "id": "ref-5",
      "number": 5,
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "publication": "ICLR",
      "year": 2015
    },
    {
      "id": "ref-6",
      "number": 6,
      "authors": [
        "Minh-Thang Luong",
        "Hieu Pham",
        "Christopher D. Manning"
      ],
      "title": "Effective Approaches to Attention-based Neural Machine Translation",
      "publication": "EMNLP",
      "year": 2015,
      "pages": "1412-1421"
    },
    {
      "id": "ref-7",
      "number": 7,
      "authors": [
        "Ondrej Bojar",
        "et al."
      ],
      "title": "Findings of the 2014 Workshop on Statistical Machine Translation",
      "publication": "WMT",
      "year": 2014
    },
    {
      "id": "ref-8",
      "number": 8,
      "authors": [
        "Rico Sennrich",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "title": "Neural Machine Translation of Rare Words with Subword Units",
      "publication": "ACL",
      "year": 2016,
      "pages": "1715-1725"
    }
  ],
  "blockNotes": [
    {
      "id": "note-1",
      "blockId": "block-math-1",
      "content": "这个公式是Transformer的核心，通过缩放点积注意力来计算权重。除以sqrt(d_k)是为了防止梯度消失。",
      "tags": [
        "核心公式",
        "注意力机制"
      ],
      "createdAt": "2025-10-03T16:20:00.000Z",
      "updatedAt": "2025-10-03T16:20:00.000Z"
    },
    {
      "id": "note-2",
      "blockId": "block-fig-1",
      "content": "这张图清楚地展示了Transformer的整体架构，包括编码器和解码器的堆叠结构。需要重点关注多头注意力层和残差连接。",
      "tags": [
        "架构图",
        "重要"
      ],
      "createdAt": "2025-10-03T17:45:00.000Z",
      "updatedAt": "2025-10-03T17:45:00.000Z"
    },
    {
      "id": "note-3",
      "blockId": "block-table-1",
      "content": "模型的超参数设置，base模型用d_model=512，big模型可能会更大。8个注意力头是一个经典配置。",
      "tags": [
        "超参数"
      ],
      "createdAt": "2025-10-04T09:15:00.000Z",
      "updatedAt": "2025-10-04T09:15:00.000Z"
    },
    {
      "id": "note-4",
      "blockId": "block-code-1",
      "content": "FFN的实现很简单，就是两层全连接网络，中间加ReLU激活和Dropout。这个结构在每个位置独立应用。",
      "tags": [
        "代码",
        "实现细节"
      ],
      "createdAt": "2025-10-04T10:00:00.000Z",
      "updatedAt": "2025-10-04T10:00:00.000Z"
    },
    {
      "id": "note-5",
      "blockId": "block-table-2",
      "content": "实验结果非常impressive！Transformer在翻译任务上超越了所有baseline，而且训练成本更低。这证明了纯注意力机制的有效性。",
      "tags": [
        "实验结果",
        "性能对比"
      ],
      "createdAt": "2025-10-04T10:25:00.000Z",
      "updatedAt": "2025-10-04T10:25:00.000Z"
    },
    {
      "id": "note-6",
      "blockId": "block-p1",
      "content": "RNN和LSTM是当时的SOTA方法，但存在并行化困难的问题。Transformer正是为了解决这个问题而提出的。",
      "tags": [
        "背景",
        "动机"
      ],
      "createdAt": "2025-10-04T10:30:00.000Z",
      "updatedAt": "2025-10-04T10:30:00.000Z"
    },
    {
      "id": "note-7",
      "blockId": "block-math-2",
      "content": "多头注意力通过concat不同的attention head来捕获不同子空间的信息，这是Transformer的一个关键创新。",
      "tags": [
        "多头注意力"
      ],
      "createdAt": "2025-10-04T10:35:00.000Z",
      "updatedAt": "2025-10-04T10:35:00.000Z"
    },
    {
      "id": "note-1759827989014-39dzywun6",
      "blockId": "block-fig-1",
      "content": "这个笔记真的能保存吗，不难吧使用**Mrkdndoin**语法，还能",
      "tags": [],
      "createdAt": "2025-10-07T09:06:29.014Z",
      "updatedAt": "2025-10-07T09:07:17.838Z"
    },
    {
      "id": "note-1759828076526-77jd242ly",
      "blockId": "block-p3",
      "content": "添加新的笔记",
      "tags": [],
      "createdAt": "2025-10-07T09:07:56.526Z",
      "updatedAt": "2025-10-07T09:07:56.526Z"
    },
    {
      "id": "note-1760075806173-zglly7hv5",
      "blockId": "block-p1",
      "content": "你好",
      "tags": [],
      "createdAt": "2025-10-10T05:56:46.173Z",
      "updatedAt": "2025-10-10T05:56:46.173Z"
    },
    {
      "id": "note-1760111622862-nur9leyp6",
      "blockId": "block-p1-2",
      "content": "哈哈",
      "tags": [
        "测试标签"
      ],
      "createdAt": "2025-10-10T15:53:42.862Z",
      "updatedAt": "2025-10-10T15:53:57.854Z"
    }
  ],
  "checklistNotes": [
    {
      "id": "note-1759987231557-bkz8bvlgp",
      "checklistId": "25ee44dc-1eca-45cf-9b9c-5b5d73edb917",
      "checklistPath": "测测/你好",
      "content": "hahaa",
      "tags": [],
      "createdAt": "2025-10-09T05:20:31.557Z",
      "updatedAt": "2025-10-09T05:20:31.557Z"
    },
    {
      "id": "note-1759987233817-8zhw3rtdb",
      "checklistId": "25ee44dc-1eca-45cf-9b9c-5b5d73edb917",
      "checklistPath": "测测/你好",
      "content": "hahaha",
      "tags": [],
      "createdAt": "2025-10-09T05:20:33.817Z",
      "updatedAt": "2025-10-09T05:20:33.817Z"
    },
    {
      "id": "note-1759987236542-i3hbo98xf",
      "checklistId": "25ee44dc-1eca-45cf-9b9c-5b5d73edb917",
      "checklistPath": "测测/你好",
      "content": "ahaha",
      "tags": [],
      "createdAt": "2025-10-09T05:20:36.542Z",
      "updatedAt": "2025-10-09T05:20:36.542Z"
    },
    {
      "id": "note-1759992783351-esivm8xaw",
      "checklistId": "25ee44dc-1eca-45cf-9b9c-5b5d73edb917",
      "checklistPath": "测测/你好",
      "content": "eae",
      "tags": [],
      "createdAt": "2025-10-09T06:53:03.351Z",
      "updatedAt": "2025-10-09T06:53:03.352Z"
    }
  ],
  "pdfPath": "/api/uploads/papers/aa25138f-98c2-4925-b68e-c48cc99334b0/transformer.pdf",
  "attachments": [
    "/api/uploads/papers/aa25138f-98c2-4925-b68e-c48cc99334b0/supplementary.pdf",
    "/api/uploads/papers/aa25138f-98c2-4925-b68e-c48cc99334b0/code.zip"
  ]
}
// backend/scripts/createTestPaper.ts

import { initDatabase } from '../src/utils/database.js';
import { initFileSystem, savePaperJSON } from '../src/utils/fileSystem.js';
import { Paper } from '../src/models/Paper.js';
import { v4 as uuidv4 } from 'uuid';

async function createTestPaper() {
  console.log('å¼€å§‹åˆ›å»ºæµ‹è¯•è®ºæ–‡...\n');

  // åˆå§‹åŒ–
  await initDatabase();
  await initFileSystem();

  const paperId = `paper-${uuidv4()}`;
  const now = new Date().toISOString();

  // 1) åˆ›å»ºæ•°æ®åº“è®°å½•ï¼ˆå…¨éƒ¨ç”¨ camelCaseï¼›ä¸è¦ç»™å¯é€‰å­—æ®µä¼  nullï¼‰
  const paperRecord = {
    id: paperId,
    title: 'Attention Is All You Need',
    shortTitle: 'Transformer',
    authors: JSON.stringify([
      { name: 'Ashish Vaswani', affiliation: 'Google Brain' },
      { name: 'Noam Shazeer', affiliation: 'Google Brain' },
      { name: 'Niki Parmar', affiliation: 'Google Research' }
    ]),
    publication: 'NeurIPS',
    year: 2017,
    date: '2017-12-04',
    doi: '10.48550/arXiv.1706.03762',
    articleType: 'conference',
    // sciQuartile: çœç•¥
    // casQuartile: çœç•¥
    ccfRank: 'A',
    // impactFactor: çœç•¥
    tags: JSON.stringify(['æ·±åº¦å­¦ä¹ ', 'Transformer', 'æ³¨æ„åŠ›æœºåˆ¶', 'NLP']),
    readingStatus: 'unread',
    priority: 'high',
    rating: 5,
    remarks: 'è¿™æ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸçš„é‡Œç¨‹ç¢‘è®ºæ–‡ï¼Œæå‡ºäº†Transformeræ¶æ„ã€‚',
    readingPosition: 0,
    totalReadingTime: 0,
    // lastReadTime: çœç•¥
    isFinished: false,
  };

  try {
    await Paper.create(paperRecord);
    console.log('âœ… æ•°æ®åº“è®°å½•åˆ›å»ºæˆåŠŸ');
  } catch (e) {
    console.error('âŒ æ•°æ®åº“å†™å…¥å¤±è´¥:', e);
    process.exit(1);
  }

  // 2) åˆ›å»º JSON æ–‡ä»¶å†…å®¹ï¼ˆmetadata ä¹Ÿç»Ÿä¸€ camelCaseï¼Œä½¿ç”¨ remarksï¼‰
  const paperContent = {
    metadata: {
      id: paperId,
      title: 'Attention Is All You Need',
      shortTitle: 'Transformer',
      authors: [
        { name: 'Ashish Vaswani', affiliation: 'Google Brain' },
        { name: 'Noam Shazeer', affiliation: 'Google Brain' },
        { name: 'Niki Parmar', affiliation: 'Google Research' }
      ],
      publication: 'NeurIPS',
      year: 2017,
      date: '2017-12-04',
      doi: '10.48550/arXiv.1706.03762',
      articleType: 'conference',
      ccfRank: 'A',
      abstract: {
        en: 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms.',
        zh: 'å½“å‰ä¸»æµçš„åºåˆ—è½¬æ¢æ¨¡å‹åŸºäºå¤æ‚çš„å¾ªç¯ç¥ç»ç½‘ç»œæˆ–å·ç§¯ç¥ç»ç½‘ç»œã€‚æ€§èƒ½æœ€å¥½çš„æ¨¡å‹è¿˜é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿æ¥ç¼–ç å™¨å’Œè§£ç å™¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç®€å•ç½‘ç»œæ¶æ„â€”â€”Transformerï¼Œå®ƒå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ã€‚'
      },
      keywords: ['æ·±åº¦å­¦ä¹ ', 'Transformer', 'æ³¨æ„åŠ›æœºåˆ¶', 'NLP'],
      tags: ['æ·±åº¦å­¦ä¹ ', 'Transformer', 'æ³¨æ„åŠ›æœºåˆ¶', 'NLP'],
      readingStatus: 'unread',
      priority: 'high',
      rating: 5,
      remarks: 'è¿™æ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸçš„é‡Œç¨‹ç¢‘è®ºæ–‡ï¼Œæå‡ºäº†Transformeræ¶æ„ã€‚',
      createdAt: now,
      updatedAt: now
    },
    sections: [
      {
        id: 'section-1',
        number: '1',
        title: { en: 'Introduction', zh: 'å¼•è¨€' },
        content: [
          {
            id: 'block-p1',
            type: 'paragraph',
            content: {
              en: [
                { type: 'text', content: 'Recurrent neural networks, long short-term memory ' },
                { type: 'citation', referenceIds: ['ref-1', 'ref-2'], displayText: '[1, 2]' },
                { type: 'text', content: ' and gated recurrent ' },
                { type: 'citation', referenceIds: ['ref-3'], displayText: '[3]' },
                { type: 'text', content: ' neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems.' }
              ],
              zh: [
                { type: 'text', content: 'å¾ªç¯ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ ' },
                { type: 'citation', referenceIds: ['ref-1', 'ref-2'], displayText: '[1, 2]' },
                { type: 'text', content: ' å’Œé—¨æ§å¾ªç¯ç¥ç»ç½‘ç»œ ' },
                { type: 'citation', referenceIds: ['ref-3'], displayText: '[3]' },
                { type: 'text', content: 'ï¼Œå·²ç»è¢«ç‰¢å›ºåœ°ç¡®ç«‹ä¸ºåºåˆ—å»ºæ¨¡å’Œè½¬æ¢é—®é¢˜çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚' }
              ]
            }
          },
          {
            id: 'block-p2',
            type: 'paragraph',
            content: {
              en: [
                { type: 'text', content: 'The Transformer model architecture is shown in ' },
                { type: 'figure-ref', figureId: 'block-fig-1', displayText: 'Figure 1' },
                { type: 'text', content: '.' }
              ],
              zh: [
                { type: 'text', content: 'Transformeræ¨¡å‹æ¶æ„å¦‚' },
                { type: 'figure-ref', figureId: 'block-fig-1', displayText: 'å›¾1' },
                { type: 'text', content: 'æ‰€ç¤ºã€‚' }
              ]
            }
          }
        ]
      },
      {
        id: 'section-2',
        number: '2',
        title: { en: 'Model Architecture', zh: 'æ¨¡å‹æ¶æ„' },
        content: [
          {
            id: 'block-fig-1',
            type: 'figure',
            number: 1,
            src: '/placeholder-transformer-architecture.png',
            alt: 'Transformer Architecture',
            caption: {
              en: [{ type: 'text', content: 'The Transformer model architecture' }],
              zh: [{ type: 'text', content: 'Transformeræ¨¡å‹æ¶æ„' }]
            },
            description: {
              en: [{ type: 'text', content: 'The encoder is on the left and the decoder is on the right.' }],
              zh: [{ type: 'text', content: 'ç¼–ç å™¨åœ¨å·¦ä¾§ï¼Œè§£ç å™¨åœ¨å³ä¾§ã€‚' }]
            },
            width: '80%'
          },
          {
            id: 'block-p3',
            type: 'paragraph',
            content: {
              en: [
                { type: 'text', content: 'Most competitive neural sequence transduction models have an encoder-decoder structure ' },
                { type: 'citation', referenceIds: ['ref-4', 'ref-5'], displayText: '[4, 5]' },
                { type: 'text', content: '. The encoder maps an input sequence to a sequence of continuous representations.' }
              ]
            }
          },
          {
            id: 'block-math-1',
            type: 'math',
            latex: '\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V',
            label: '(1)',
            number: 1
          }
        ]
      }
    ],
    references: [
      { id: 'ref-1', number: 1, authors: ['Sepp Hochreiter', 'JÃ¼rgen Schmidhuber'], title: 'Long Short-Term Memory', publication: 'Neural Computation', year: 1997, volume: '9', issue: '8', pages: '1735-1780' },
      { id: 'ref-2', number: 2, authors: ['Kyunghyun Cho', 'et al.'], title: 'Learning Phrase Representations using RNN Encoder-Decoder', publication: 'EMNLP', year: 2014 },
      { id: 'ref-3', number: 3, authors: ['Junyoung Chung', 'et al.'], title: 'Empirical Evaluation of Gated Recurrent Neural Networks', year: 2014 },
      { id: 'ref-4', number: 4, authors: ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V. Le'], title: 'Sequence to Sequence Learning with Neural Networks', publication: 'NIPS', year: 2014 },
      { id: 'ref-5', number: 5, authors: ['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio'], title: 'Neural Machine Translation by Jointly Learning to Align and Translate', publication: 'ICLR', year: 2015 }
    ],
    blockNotes: [],
    checklistNotes: []
  };

  await savePaperJSON(paperId, paperContent);
  console.log('âœ… JSONæ–‡ä»¶åˆ›å»ºæˆåŠŸ');

  console.log(`\nâœ¨ æµ‹è¯•è®ºæ–‡åˆ›å»ºå®Œæˆï¼`);
  console.log(`ğŸ“ è®ºæ–‡ID: ${paperId}`);
  console.log(`ğŸ“„ æ ‡é¢˜: ${paperRecord.title}`);
  console.log(`\nå¯ä»¥é€šè¿‡ä»¥ä¸‹APIè®¿é—®:`);
  console.log(`   GET  http://localhost:3001/api/papers/${paperId}`);
  console.log(`   GET  http://localhost:3001/api/papers/${paperId}/content`);
}

// è¿è¡Œè„šæœ¬
createTestPaper()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('âŒ åˆ›å»ºå¤±è´¥:', error);
    process.exit(1);
  });
